{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for data analysis TP 5 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1) Machine Learning ?\n",
    "\n",
    "Machine Learning algorithms are an application of artificial intelligence (AI) that automatically build a mathematical model using sample data – also known as “training data” – to make decisions without being specifically programmed to make those decisions.\n",
    "\n",
    "\n",
    "\n",
    "### 2) Supervised\n",
    "\n",
    "On donne à la machine un lot d'exemples labelisés. La machine doit trouver une loi permettant de déduire l'output à partir des inputs.\n",
    "\n",
    "Classification  ( outputs discrets)\n",
    "- Spam / no spam ?\n",
    "- malade / pas malade ?\n",
    "- 1/2/3/4/5/6/7/8 ou 9 ?\n",
    "- cliquera / cliquera pas ?\n",
    "\n",
    "Mais aussi Regression (outputs continus)\n",
    "\n",
    "- Valeur d'une maison en fonction de sa surface ?\n",
    "- Nb de nouveaux cas covid en fonction de la température ?\n",
    "\n",
    "\n",
    "\n",
    "Entraîner (faire apprendre) un modèle de régression c’est faire du ML. \n",
    "\n",
    "\"Le ML peut être vu comme l’ensemble des méthodes permettant de déterminer la meilleure manière de modéliser les données. Parmi ces méthodes, la plus centrale et la plus importante est l’apprentissage. L’apprentissage consiste, pour un modèle donné, à choisir les meilleurs paramètres possibles pour décrire les données. Pour une régression simple, avec une droite y=ax+b, il s’agit de choisir le meilleur couple a, b. Dans ce cas, la connaissance acquise par l’expérience (les différents échantillons de données sont autant de micro-expériences), est stockée dans ces deux coefficients a et b. Un modèle plus complexe stocke sa connaissance dans un nombre beaucoup plus grand de coefficients pouvant dépasser le million voire le milliard (pour les réseaux de neurones complexes par exemple), mais le principe est le même : le ML permet à une machine (le modèle, simple ou complexe) d’apprendre et de stocker sa compréhension du monde.\"\n",
    "\n",
    "( https://www.quantmetry.com/blog/une-petite-histoire-du-machine-learning/ ) \n",
    "\n",
    "\n",
    "\n",
    "### 3) Unsupervised\n",
    "\n",
    "On ne fournit aucun label à l'algorithme. Il doit découvrir sans assitance les structures caractéristiques dans les données. \n",
    "Clustering : on transmet un lot d'images représentant tous types d'objet, l'algorithme doit repérer la répétition d'objets similaires et trier les images en fonction: immeubles, chiens, arbres, voitures. Bien sur il ne connaitra pas le nom des objets qu'il identifie, mais il comprendra qu'ils sont similaires et pourra leur donner le meme label.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Reinforcement learning : \n",
    "L'algo qui a appris de façon supervisée est capable de se renforcer en jouant ( alphago), ou 100% RL pour la dernière version.\n",
    "L'algo se renforce en recevant une récompense, positive ou négative: alphago , robots boston dynamics : récompense forte si il reste debout, négative quand il tombe. Il explore lui même les différents mouvements et en déduit les meilleurs, ceux qui lui permettent de rester debout.\n",
    "\n",
    "\n",
    "\n",
    "### 3)  The stages of a model's life\n",
    "\n",
    "\n",
    "- 1. Collecte des données\n",
    "- 2. Analyse descriptive pour comprendre les données\n",
    "- 3. Transformation si besoin ( bucketisation, simplification, centrage..)\n",
    "- 4. Division en training set et testing set\n",
    "- 5. Choix Modèle\n",
    "- 6. Training à partir du training set  <------\n",
    "- 7. Evaluation sur test set                  |\n",
    "- 8. Modification des Hyper-paramètres --------\n",
    "- 9. Utilisation / mise en production\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 3) Many available ML Algorithms : \n",
    "\n",
    "- Support Vector Machines\n",
    "- Linear Regression\n",
    "- Logistic Regression (with threshold)\n",
    "- Catboost\n",
    "- Random forest\n",
    "- Classification trees\n",
    "- boosting\n",
    "- k-nearest neighbors\n",
    "- Naive Bayes\n",
    "...\n",
    "\n",
    "more details here : https://www.dataquest.io/blog/top-10-machine-learning-algorithms-for-beginners/\n",
    "\n",
    "pour les maths afférentes à ces modèles : https://stanford.edu/~shervine/l/fr/teaching/cs-229/pense-bete-apprentissage-supervise\n",
    "\n",
    "\n",
    "### 4)  Evaluation des résultats\n",
    "\n",
    "We need some evaluation methods to be able to determine if our trained model is doing a good job.\n",
    "\n",
    "The simplest metric that comes to mind is Accuracy : total number of good guess / 100 examples\n",
    "\n",
    "This method can be sufficient in many cases, but not all.\n",
    "Imagine a situation where you have in real life 99,9% negative and 0,1 positive, but you want to be confident in model that will detect this 0,1 positive. Training a model with an accuracy of 99,9 % will not do the job ! \n",
    "for instance : tumor detection. you want to minimize false negatives.\n",
    "\n",
    "You need other validation metrics \n",
    "\n",
    "\n",
    "Model evaluation methods : \n",
    "\n",
    "https://stanford.edu/~shervine/l/fr/teaching/cs-229/pense-bete-machine-learning-petites-astuces\n",
    "\n",
    "Classification Matrix : accuracy(% good guesses), precision (% of positive predicton that were true), recall (TPR) , specificy (TNR)\n",
    "ROC ( TP / FP for all thresholds)\n",
    "Area under roc = probability a random positive example will get a higher score than a random negative example\n",
    "\n",
    "( slides google ML)\n",
    "\n",
    "\n",
    "Model Pitfalls\n",
    "(https://towardsdatascience.com/machine-learning-pitfalls-e54ac3edc25)\n",
    "\n",
    "- Correlation is not Causation\n",
    "Spurious correlation : shark attacks and ice cream. \n",
    "pointure et niveau en maths chez les collégiens.\n",
    "Cum hoc ergo propter hoc\n",
    "\n",
    "\n",
    "\n",
    "- Underfitting / Overfitting\n",
    "\n",
    "- Unrepresentative dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "( standard scaler : https://towardsdatascience.com/how-and-why-to-standardize-your-data-996926c2c832 ) \n",
    "\n",
    "\n",
    "### 5) THE SCIKIT LEARN WAY\n",
    "\n",
    "\n",
    "\"Scikit-Learn is characterized by a clean, uniform, and streamlined API, as well as by very useful and complete online documentation. A benefit of this uniformity is that once you understand the basic use and syntax of Scikit-Learn for one type of model, switching to a new model or algorithm is very straightforward.\"\n",
    "\n",
    "\n",
    "Scikit-learn provides dozens of built-in machine learning algorithms and models, called estimators. Each estimator can be fitted to some data using its fit method.\n",
    "\n",
    "( https://scikit-learn.org/stable/getting_started.html )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1) Simple Classification using RandomForest algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that **operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees**.[1][2] Random decision forests correct for decision trees' habit of overfitting to their training set.[3]:587–588 Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees. However, data characteristics can affect their performance.\n",
    "\n",
    "\n",
    "Random forests are frequently used as \"blackbox\" models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration in packages such as scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [11, 12, 13]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Choice , Model Import\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# model (classifier) instanciation, tuning hyper parameters\n",
    "# This algorithm need some random numbers to start\n",
    "\n",
    "# since we're giving a fixed random_state, the classifier should behave the same every time\n",
    "clf = RandomForestClassifier(random_state=0) \n",
    "\n",
    "\n",
    "# Creation of the features matrix : 1 sample / line, 1 variable/feature /column\n",
    "X = [[ 1,  2,  3], [11, 12, 13]]  # 2 samples, 3 features\n",
    "\n",
    "# The samples (i.e., rows) always refer to the individual objects described by the dataset.\n",
    "# a flower, an email, an image, a person..\n",
    "\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creation of the target array  : one label/class / line\n",
    "# The array must contains the same nb of lines as the features matrix\n",
    "y = [0, 1]  # classes of each sample\n",
    "\n",
    "# Fit the model to your data ( training)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now that the model is trained, we can use it to make predictions on unseen data :\n",
    "\n",
    "clf.predict(X)  # predict classes of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[4, 5, 12], [14, 15, 16]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[1, 7, 10]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "# Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of SVM or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. – seralouk May 17 at 10:18 \n",
    "# So, Standardization leads to a) more stable b) less influenced by the range of variables c) faster fitting d) more stable performance\n",
    "# from https://stackoverflow.com/questions/40758562/can-anyone-explain-me-standardscaler#:~:text=The%20idea%20behind%20StandardScaler%20is,each%20column%20of%20the%20data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2) Simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = 10 * rng.rand(50)\n",
    "y = 2 * x + rng.randn(50)\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Choice , Model Import\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# model instanciation, tuning hyper parameters\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to modify x to make it a matrix of size [n_samples, n_features]. \n",
    "X = x[:, np.newaxis]\n",
    "#  numpy.newaxis is used to increase the dimension of the existing array by one more dimension\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to your data\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW THE MODEL IS TRAINED\n",
    "# In Scikit-Learn, by convention all model parameters that were learned during the fit() process have trailing underscores; \n",
    "# for example in this linear model, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.11477878])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5885266391775215"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.75510204],\n",
       "       [-0.51020408],\n",
       "       [-0.26530612],\n",
       "       [-0.02040816],\n",
       "       [ 0.2244898 ],\n",
       "       [ 0.46938776],\n",
       "       [ 0.71428571],\n",
       "       [ 0.95918367],\n",
       "       [ 1.20408163],\n",
       "       [ 1.44897959],\n",
       "       [ 1.69387755],\n",
       "       [ 1.93877551],\n",
       "       [ 2.18367347],\n",
       "       [ 2.42857143],\n",
       "       [ 2.67346939],\n",
       "       [ 2.91836735],\n",
       "       [ 3.16326531],\n",
       "       [ 3.40816327],\n",
       "       [ 3.65306122],\n",
       "       [ 3.89795918],\n",
       "       [ 4.14285714],\n",
       "       [ 4.3877551 ],\n",
       "       [ 4.63265306],\n",
       "       [ 4.87755102],\n",
       "       [ 5.12244898],\n",
       "       [ 5.36734694],\n",
       "       [ 5.6122449 ],\n",
       "       [ 5.85714286],\n",
       "       [ 6.10204082],\n",
       "       [ 6.34693878],\n",
       "       [ 6.59183673],\n",
       "       [ 6.83673469],\n",
       "       [ 7.08163265],\n",
       "       [ 7.32653061],\n",
       "       [ 7.57142857],\n",
       "       [ 7.81632653],\n",
       "       [ 8.06122449],\n",
       "       [ 8.30612245],\n",
       "       [ 8.55102041],\n",
       "       [ 8.79591837],\n",
       "       [ 9.04081633],\n",
       "       [ 9.28571429],\n",
       "       [ 9.53061224],\n",
       "       [ 9.7755102 ],\n",
       "       [10.02040816],\n",
       "       [10.26530612],\n",
       "       [10.51020408],\n",
       "       [10.75510204],\n",
       "       [11.        ]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now that the model is trained, we can use it to make predictions on unseen data.\n",
    "# let's generate some data : \n",
    "xfit = np.linspace(-1, 11)\n",
    "\n",
    "# as before, we need to adapt the data to give the model a 2d matrix [n_samples, n_features] : \n",
    "Xfit = xfit[:, np.newaxis]\n",
    "\n",
    "Xfit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.70330542, -2.18540041, -1.6674954 , -1.1495904 , -0.63168539,\n",
       "       -0.11378038,  0.40412462,  0.92202963,  1.43993464,  1.95783965,\n",
       "        2.47574465,  2.99364966,  3.51155467,  4.02945967,  4.54736468,\n",
       "        5.06526969,  5.58317469,  6.1010797 ,  6.61898471,  7.13688972,\n",
       "        7.65479472,  8.17269973,  8.69060474,  9.20850974,  9.72641475,\n",
       "       10.24431976, 10.76222477, 11.28012977, 11.79803478, 12.31593979,\n",
       "       12.83384479, 13.3517498 , 13.86965481, 14.38755981, 14.90546482,\n",
       "       15.42336983, 15.94127484, 16.45917984, 16.97708485, 17.49498986,\n",
       "       18.01289486, 18.53079987, 19.04870488, 19.56660989, 20.08451489,\n",
       "       20.6024199 , 21.12032491, 21.63822991, 22.15613492, 22.67403993])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's feed the data to the model and store his predictions into a new array\n",
    "yfit = model.predict(Xfit)\n",
    "yfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3) Simple classification using sklearn builtin datasets, and naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given a model trained on a portion of a dataset of labeled samples, how well can we predict the remaining labels?\n",
    "\n",
    "scikit learn have built in datasets, let's use one for this example : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR',\n",
       " 'data',\n",
       " 'feature_names',\n",
       " 'filename',\n",
       " 'frame',\n",
       " 'target',\n",
       " 'target_names']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(iris) # show all attributes of an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Correlation above is Pearson's correlation coefficient. It shows linear relationship between a feature and classes.\n",
    "\n",
    "https://stackoverflow.com/questions/52905164/class-correlation-and-its-effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data.shape[0],iris.target.shape[0] # same nb of lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iris.data and iris.target numpy arrays have the same number of lines.\n",
    "\n",
    "data is the **feature matrix** (or samples matrix) , target is the 1D **target array** (or target vector / values / classes)\n",
    "\n",
    "data contains **samples**, target contains the **labels** that have been associated to these samples.\n",
    "\n",
    "it is mandatory to feed the model with a features matrix and a target array that have the same nb of lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setosa', 'versicolor', 'virginica']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "     target  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "..      ...  \n",
       "145       2  \n",
       "146       2  \n",
       "147       2  \n",
       "148       2  \n",
       "149       2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit learns accepts array-like data :  numpy arrays, lists of numbers, pandas DataFrames with all columns numeric, numeric pandas.Series ..\n",
    "# let's use pandas dataframe to present the data\n",
    "\n",
    "import pandas as pd\n",
    "df           = pd.DataFrame(iris['data'])\n",
    "df.columns   = iris['feature_names']\n",
    "df['target'] = iris['target']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.819232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.057333           3.758000   \n",
       "std             0.828066          0.435866           1.765298   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)      target  \n",
       "count        150.000000  150.000000  \n",
       "mean           1.199333    1.000000  \n",
       "std            0.762238    0.819232  \n",
       "min            0.100000    0.000000  \n",
       "25%            0.300000    0.000000  \n",
       "50%            1.300000    1.000000  \n",
       "75%            1.800000    2.000000  \n",
       "max            2.500000    2.000000  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=iris.data # X is standard name for features matrix\n",
    "y=iris.target # y is standard name for target array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((112, 4), (38, 4))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We would like to split our data into training set and testing set\n",
    "# training set will be feed to the model, with labels, for training\n",
    "\n",
    "# testing set will be used for testing, after the model have been trained\n",
    "# we will ask the model to predict labels on the testing samples, and will compare it to the testing labels\n",
    "# for this we need to split the dataset into training set and testing set\n",
    "# we could do this by hand, but scikt learn gives us a useful function to do it \n",
    "# default size of test set is 25% of original data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=1)\n",
    "\n",
    "Xtrain.shape, Xtest.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.33\n"
     ]
    }
   ],
   "source": [
    "# quelle est la proportion test set / dataset initial par défaut ?\n",
    "print (f\"{Xtest.shape[0]/( Xtrain.shape[0] +Xtest.shape[0] )*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use an extremely simple generative model known as Gaussian naive Bayes\n",
    "# Because it is so fast and has no hyperparameters to choose, \n",
    "# Gaussian naive Bayes is often a good model to use as a baseline classification, \n",
    "# before exploring whether improvements can be found through more sophisticated models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB # 1. choose model class\n",
    "model = GaussianNB()                       # 2. instantiate model\n",
    "model.fit(Xtrain, ytrain)                  # 3. fit model to data\n",
    "y_model = model.predict(Xtest)             # 4. predict on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, y_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy ?\n",
    "# nb of good predictions / nb of testing samples\n",
    "# 97% is a very good score !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4) Fast correlation matrix on the boston dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # conventional alias\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['target'] = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  target  \n",
       "0       15.3  396.90   4.98    24.0  \n",
       "1       17.8  396.90   9.14    21.6  \n",
       "2       17.8  392.83   4.03    34.7  \n",
       "3       18.7  394.63   2.94    33.4  \n",
       "4       18.7  396.90   5.33    36.2  \n",
       "..       ...     ...    ...     ...  \n",
       "501     21.0  391.99   9.67    22.4  \n",
       "502     21.0  396.90   9.08    20.6  \n",
       "503     21.0  396.90   5.64    23.9  \n",
       "504     21.0  393.45   6.48    22.0  \n",
       "505     21.0  396.90   7.88    11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677083</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT      target  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.462057</td>\n",
       "      <td>0.521014</td>\n",
       "      <td>0.033948</td>\n",
       "      <td>0.603361</td>\n",
       "      <td>-0.211718</td>\n",
       "      <td>0.497297</td>\n",
       "      <td>-0.539878</td>\n",
       "      <td>0.563969</td>\n",
       "      <td>0.544956</td>\n",
       "      <td>0.312768</td>\n",
       "      <td>-0.264378</td>\n",
       "      <td>0.454837</td>\n",
       "      <td>-0.403964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN</th>\n",
       "      <td>-0.462057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.535468</td>\n",
       "      <td>-0.039419</td>\n",
       "      <td>-0.511464</td>\n",
       "      <td>0.278134</td>\n",
       "      <td>-0.429389</td>\n",
       "      <td>0.478524</td>\n",
       "      <td>-0.234663</td>\n",
       "      <td>-0.289911</td>\n",
       "      <td>-0.361607</td>\n",
       "      <td>0.128177</td>\n",
       "      <td>-0.386818</td>\n",
       "      <td>0.339989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS</th>\n",
       "      <td>0.521014</td>\n",
       "      <td>-0.535468</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.075889</td>\n",
       "      <td>0.612030</td>\n",
       "      <td>-0.291318</td>\n",
       "      <td>0.489070</td>\n",
       "      <td>-0.565137</td>\n",
       "      <td>0.353967</td>\n",
       "      <td>0.483228</td>\n",
       "      <td>0.336612</td>\n",
       "      <td>-0.192017</td>\n",
       "      <td>0.465980</td>\n",
       "      <td>-0.418430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>0.033948</td>\n",
       "      <td>-0.039419</td>\n",
       "      <td>0.075889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056387</td>\n",
       "      <td>0.048080</td>\n",
       "      <td>0.055616</td>\n",
       "      <td>-0.065619</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>-0.037655</td>\n",
       "      <td>-0.115694</td>\n",
       "      <td>-0.033277</td>\n",
       "      <td>-0.041344</td>\n",
       "      <td>0.115202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>0.603361</td>\n",
       "      <td>-0.511464</td>\n",
       "      <td>0.612030</td>\n",
       "      <td>0.056387</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.215633</td>\n",
       "      <td>0.589608</td>\n",
       "      <td>-0.683930</td>\n",
       "      <td>0.434828</td>\n",
       "      <td>0.453258</td>\n",
       "      <td>0.278678</td>\n",
       "      <td>-0.202430</td>\n",
       "      <td>0.452005</td>\n",
       "      <td>-0.394995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>-0.211718</td>\n",
       "      <td>0.278134</td>\n",
       "      <td>-0.291318</td>\n",
       "      <td>0.048080</td>\n",
       "      <td>-0.215633</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.187611</td>\n",
       "      <td>0.179801</td>\n",
       "      <td>-0.076569</td>\n",
       "      <td>-0.190532</td>\n",
       "      <td>-0.223194</td>\n",
       "      <td>0.032951</td>\n",
       "      <td>-0.468231</td>\n",
       "      <td>0.482829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>0.497297</td>\n",
       "      <td>-0.429389</td>\n",
       "      <td>0.489070</td>\n",
       "      <td>0.055616</td>\n",
       "      <td>0.589608</td>\n",
       "      <td>-0.187611</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.609836</td>\n",
       "      <td>0.306201</td>\n",
       "      <td>0.360311</td>\n",
       "      <td>0.251857</td>\n",
       "      <td>-0.154056</td>\n",
       "      <td>0.485359</td>\n",
       "      <td>-0.387758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS</th>\n",
       "      <td>-0.539878</td>\n",
       "      <td>0.478524</td>\n",
       "      <td>-0.565137</td>\n",
       "      <td>-0.065619</td>\n",
       "      <td>-0.683930</td>\n",
       "      <td>0.179801</td>\n",
       "      <td>-0.609836</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.361892</td>\n",
       "      <td>-0.381988</td>\n",
       "      <td>-0.223486</td>\n",
       "      <td>0.168631</td>\n",
       "      <td>-0.409347</td>\n",
       "      <td>0.313115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD</th>\n",
       "      <td>0.563969</td>\n",
       "      <td>-0.234663</td>\n",
       "      <td>0.353967</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.434828</td>\n",
       "      <td>-0.076569</td>\n",
       "      <td>0.306201</td>\n",
       "      <td>-0.361892</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.558107</td>\n",
       "      <td>0.251913</td>\n",
       "      <td>-0.214364</td>\n",
       "      <td>0.287943</td>\n",
       "      <td>-0.248115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX</th>\n",
       "      <td>0.544956</td>\n",
       "      <td>-0.289911</td>\n",
       "      <td>0.483228</td>\n",
       "      <td>-0.037655</td>\n",
       "      <td>0.453258</td>\n",
       "      <td>-0.190532</td>\n",
       "      <td>0.360311</td>\n",
       "      <td>-0.381988</td>\n",
       "      <td>0.558107</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.287769</td>\n",
       "      <td>-0.241606</td>\n",
       "      <td>0.384191</td>\n",
       "      <td>-0.414650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO</th>\n",
       "      <td>0.312768</td>\n",
       "      <td>-0.361607</td>\n",
       "      <td>0.336612</td>\n",
       "      <td>-0.115694</td>\n",
       "      <td>0.278678</td>\n",
       "      <td>-0.223194</td>\n",
       "      <td>0.251857</td>\n",
       "      <td>-0.223486</td>\n",
       "      <td>0.251913</td>\n",
       "      <td>0.287769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.042152</td>\n",
       "      <td>0.330335</td>\n",
       "      <td>-0.398789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>-0.264378</td>\n",
       "      <td>0.128177</td>\n",
       "      <td>-0.192017</td>\n",
       "      <td>-0.033277</td>\n",
       "      <td>-0.202430</td>\n",
       "      <td>0.032951</td>\n",
       "      <td>-0.154056</td>\n",
       "      <td>0.168631</td>\n",
       "      <td>-0.214364</td>\n",
       "      <td>-0.241606</td>\n",
       "      <td>-0.042152</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.145430</td>\n",
       "      <td>0.126955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>0.454837</td>\n",
       "      <td>-0.386818</td>\n",
       "      <td>0.465980</td>\n",
       "      <td>-0.041344</td>\n",
       "      <td>0.452005</td>\n",
       "      <td>-0.468231</td>\n",
       "      <td>0.485359</td>\n",
       "      <td>-0.409347</td>\n",
       "      <td>0.287943</td>\n",
       "      <td>0.384191</td>\n",
       "      <td>0.330335</td>\n",
       "      <td>-0.145430</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.668656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>-0.403964</td>\n",
       "      <td>0.339989</td>\n",
       "      <td>-0.418430</td>\n",
       "      <td>0.115202</td>\n",
       "      <td>-0.394995</td>\n",
       "      <td>0.482829</td>\n",
       "      <td>-0.387758</td>\n",
       "      <td>0.313115</td>\n",
       "      <td>-0.248115</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>0.126955</td>\n",
       "      <td>-0.668656</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "CRIM     1.000000 -0.462057  0.521014  0.033948  0.603361 -0.211718  0.497297   \n",
       "ZN      -0.462057  1.000000 -0.535468 -0.039419 -0.511464  0.278134 -0.429389   \n",
       "INDUS    0.521014 -0.535468  1.000000  0.075889  0.612030 -0.291318  0.489070   \n",
       "CHAS     0.033948 -0.039419  0.075889  1.000000  0.056387  0.048080  0.055616   \n",
       "NOX      0.603361 -0.511464  0.612030  0.056387  1.000000 -0.215633  0.589608   \n",
       "RM      -0.211718  0.278134 -0.291318  0.048080 -0.215633  1.000000 -0.187611   \n",
       "AGE      0.497297 -0.429389  0.489070  0.055616  0.589608 -0.187611  1.000000   \n",
       "DIS     -0.539878  0.478524 -0.565137 -0.065619 -0.683930  0.179801 -0.609836   \n",
       "RAD      0.563969 -0.234663  0.353967  0.021739  0.434828 -0.076569  0.306201   \n",
       "TAX      0.544956 -0.289911  0.483228 -0.037655  0.453258 -0.190532  0.360311   \n",
       "PTRATIO  0.312768 -0.361607  0.336612 -0.115694  0.278678 -0.223194  0.251857   \n",
       "B       -0.264378  0.128177 -0.192017 -0.033277 -0.202430  0.032951 -0.154056   \n",
       "LSTAT    0.454837 -0.386818  0.465980 -0.041344  0.452005 -0.468231  0.485359   \n",
       "target  -0.403964  0.339989 -0.418430  0.115202 -0.394995  0.482829 -0.387758   \n",
       "\n",
       "              DIS       RAD       TAX   PTRATIO         B     LSTAT    target  \n",
       "CRIM    -0.539878  0.563969  0.544956  0.312768 -0.264378  0.454837 -0.403964  \n",
       "ZN       0.478524 -0.234663 -0.289911 -0.361607  0.128177 -0.386818  0.339989  \n",
       "INDUS   -0.565137  0.353967  0.483228  0.336612 -0.192017  0.465980 -0.418430  \n",
       "CHAS    -0.065619  0.021739 -0.037655 -0.115694 -0.033277 -0.041344  0.115202  \n",
       "NOX     -0.683930  0.434828  0.453258  0.278678 -0.202430  0.452005 -0.394995  \n",
       "RM       0.179801 -0.076569 -0.190532 -0.223194  0.032951 -0.468231  0.482829  \n",
       "AGE     -0.609836  0.306201  0.360311  0.251857 -0.154056  0.485359 -0.387758  \n",
       "DIS      1.000000 -0.361892 -0.381988 -0.223486  0.168631 -0.409347  0.313115  \n",
       "RAD     -0.361892  1.000000  0.558107  0.251913 -0.214364  0.287943 -0.248115  \n",
       "TAX     -0.381988  0.558107  1.000000  0.287769 -0.241606  0.384191 -0.414650  \n",
       "PTRATIO -0.223486  0.251913  0.287769  1.000000 -0.042152  0.330335 -0.398789  \n",
       "B        0.168631 -0.214364 -0.241606 -0.042152  1.000000 -0.145430  0.126955  \n",
       "LSTAT   -0.409347  0.287943  0.384191  0.330335 -0.145430  1.000000 -0.668656  \n",
       "target   0.313115 -0.248115 -0.414650 -0.398789  0.126955 -0.668656  1.000000  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr=df.corr(method='kendall')\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2f5ca88cc40>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn # just a conventional alias, don't know why\n",
    "import numpy as np\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = seaborn.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "seaborn.heatmap(corr, mask=mask,cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(more visualization : http://www.neural.cz/dataset-exploration-boston-house-pricing.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5) Hyperparameters, Model Validation and Testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# we choose a model and hyperparameters. \n",
    "# Here we'll use a k-neighbors classifier with n_neighbors=1. \n",
    "# This is a very simple and intuitive model that says \n",
    "# \"the label of an unknown point is the same as the label of its closest training point:\"\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# n_neighbors is the hyperparameter\n",
    "\n",
    "model.fit(X, y)\n",
    "y_model = model.predict(X)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, y_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**100% ?? Really ?**\n",
    "\n",
    "Yes but.. We gave all our samples to train the model. We know that the model is perfectly trained on our sample data but.. we have no idea how it will behave on new data !\n",
    "\n",
    "1. You should never train and validate the model using the same data. As we did before, we must split the data into distinct training set and test set, so we will be able to test our model on unseen data.\n",
    "\n",
    "2. \"Furthermore, the nearest neighbor model is an instance-based estimator that simply stores the training data, and predicts labels by comparing new data to these stored points: except in contrived cases, it will get 100% accuracy every time!\"\n",
    "(Jake VanderPlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9222222222222223"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data with 50% in each set\n",
    "X1, X2, y1, y2 = train_test_split(X, y, random_state=0, train_size=0.4)\n",
    "\n",
    "\n",
    "# fit the model on one set of data\n",
    "model.fit(X1, y1)\n",
    "\n",
    "# evaluate the model on the second set of data\n",
    "y2_model = model.predict(X2)\n",
    "accuracy_score(y2, y2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9222222222222223"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X2,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97368421, 0.92105263, 0.94594595, 1.        ])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=4)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, the score computed at each CV iteration is the score method of the estimator. It is possible to change this by using the scoring parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with different values of training set size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6) Over-Fitting, Cross Validation\n",
    "\n",
    "( https://scikit-learn.org/stable/modules/cross_validation.html )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The situation we were in, when we train the model on 100% of the dataset is called **Overfitting**\n",
    "\n",
    "overfitting is caused by making a model more complex than necessary. The fundamental tension of machine learning is between fitting our data well, but also fitting the data as simply as possible.\n",
    "\n",
    "(Ockham's razor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](overfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also overfit when we train too much our model for our training set, by making more iterations for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](overfitting_error.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now we didn't had to tune our models before training.\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include C, kernel and gamma for Support Vector Classifier, alpha for Lasso, etc.\n",
    "\n",
    "In machine learning, a hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are derived via training.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)\n",
    "\n",
    "Model Hyperparameter tuning is very useful to enhance the performance of a machine learning model. We have discussed both the approaches to do the tuning that is GridSearchCV and RandomizedSeachCV. The only difference between both the approaches is in grid search we define the combinations and do training of the model whereas in RandomizedSearchCV the model selects the combinations randomly. Both are very effective ways of tuning the parameters that increase the model generalizability. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4) Working with images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour aller plus loin\n",
    "\n",
    "Le ML est un sujet très riche. N'hésitez pas à accumuler les explications pour renforcer vos connaissances.\n",
    "\n",
    "Je vous conseille de suivre les livres / cours / tutos suivants :\n",
    "\n",
    "- Google ML crash course\n",
    "- Kaggle intro to machine learning\n",
    "- Chapitre 5 de Python for datascience Handbook de Jake VanderPlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOURCES USED IN THIS NOTEBOOK\n",
    "\n",
    "- Romain jouin pdf and notebooks\n",
    "- jakevdp oreilly book\n",
    "- scikit-learn getting started\n",
    "- and Stackoverflow... as always.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
